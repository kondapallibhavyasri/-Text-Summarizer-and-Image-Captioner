{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JhK02Zu6Q1Pj",
        "outputId": "54ca944c-4f16-4dde-ad44-9668fa25d462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.nn import functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "g-8pCSc_cTdc"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(256)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "block_size        = 40      ## N tokens in sequence\n",
        "batch_size        = 64\n",
        "max_iters         = 6000\n",
        "eval_interval     = 500\n",
        "learning_rate     = 0.0003\n",
        "eval_iters        = 300\n",
        "vocab_size        = 65\n",
        "\n",
        "## every id for a given token is embedded to vector of this size\n",
        "n_embd            = 512\n",
        "n_head            = 8         ## 8 attention heads\n",
        "n_layer           = 6         ## 6 eoncoder layers\n",
        "dropout           = 0.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "J1bcmdsIcW3L",
        "outputId": "41c3e03d-dafd-4faa-83f1-500bbbf703a9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4ca13de2-1028-4c5e-b15b-548c25a9ef6c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4ca13de2-1028-4c5e-b15b-548c25a9ef6c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Trained_newsroom_dataset.csv to Trained_newsroom_dataset (2).csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "with open('/content/Trained_newsroom_dataset.csv', 'r', encoding='utf-8') as f:\n",
        "    text = f.read().split('.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "35PTBHnPcY5p"
      },
      "outputs": [],
      "source": [
        "# Initialize tiktoken encoder/decoder\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # using the GPT-3 base encoding\n",
        "encode = tokenizer.encode\n",
        "decode = tokenizer.decode\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ch09P_7MccEQ",
        "outputId": "555a4fac-f8d6-4ef1-8fa3-d0059650a301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded text length: 79748\n",
            "Sample content: title,link,date\n",
            "Purdue scientist expecting new world to reveal itself to Mars rover,https://www.purdue.edu/newsroom/2024/Q4/purdue-scientist-expecting-new-world-to-reveal-itself-to-mars-rover,\"December 3, 2024\"\n",
            "Participants in the Purdue Ukrainian Scholars Initiative to be featured in annual panel discussion,https://www.purdue.edu/newsroom/2024/Q4/participants-in-the-purdue-ukrainian-scholars-initiative-to-be-featured-in-annual-panel-discussion,\"December 3, 2024\"\n",
            "\"Cement grows stronger, more res\n"
          ]
        }
      ],
      "source": [
        "text = ''\n",
        "\n",
        "input_file2 = '/content/purdue_news_all_pages-2024.csv'\n",
        "\n",
        "with open(input_file2, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(f\"Loaded text length: {len(text)}\")\n",
        "print(f\"Sample content: {text[:500]}\")\n",
        "# Tokenize the text\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FhSBBhbccddD",
        "outputId": "d910fde5-213e-4c47-c6d4-8790c043f982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of data in letter or characters\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79748"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "print(\"length of data in letter or characters\")\n",
        "len(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "i386KFvOcfPH",
        "outputId": "ad2ce408-e6f0-4cc9-fba5-86679e6be2b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['5',\n",
              " 'J',\n",
              " 'x',\n",
              " '\\xa0',\n",
              " 'j',\n",
              " 'q',\n",
              " '‘',\n",
              " 'è',\n",
              " 'U',\n",
              " '$',\n",
              " 'B',\n",
              " '.',\n",
              " 'L',\n",
              " 'I',\n",
              " 'p',\n",
              " 'g',\n",
              " '7',\n",
              " ' ',\n",
              " ';',\n",
              " 'S',\n",
              " 'u',\n",
              " ':',\n",
              " 'P',\n",
              " 'K',\n",
              " 'N',\n",
              " 'c',\n",
              " 'r',\n",
              " '4',\n",
              " '/',\n",
              " '2',\n",
              " '3',\n",
              " 'e',\n",
              " '&',\n",
              " 'Q',\n",
              " 't',\n",
              " 'R',\n",
              " 'O',\n",
              " ',',\n",
              " '?',\n",
              " '9',\n",
              " 's',\n",
              " 'a',\n",
              " 'l',\n",
              " 'H',\n",
              " 'w',\n",
              " 'M',\n",
              " 'A',\n",
              " 'h',\n",
              " 'T',\n",
              " 'G',\n",
              " '6',\n",
              " '-',\n",
              " '—',\n",
              " 'i',\n",
              " 'E',\n",
              " 'v',\n",
              " '0',\n",
              " '8',\n",
              " 'k',\n",
              " 'z',\n",
              " 'b',\n",
              " 'C',\n",
              " 'Y',\n",
              " '\"',\n",
              " 'd',\n",
              " 'o',\n",
              " 'f',\n",
              " '\\n',\n",
              " 'm',\n",
              " 'F',\n",
              " '’',\n",
              " 'V',\n",
              " 'W',\n",
              " '–',\n",
              " '%',\n",
              " 'n',\n",
              " '1',\n",
              " 'D',\n",
              " 'y']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "list(set(text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sjNEImDDchC_",
        "outputId": "2e0069e9-0573-4339-bcd8-7fdc7b0e0aa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79\n",
            "\n",
            " \"$%&,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWYabcdefghijklmnopqrstuvwxyz è–—‘’\n"
          ]
        }
      ],
      "source": [
        "the_chars  = sorted(     list(set(text))     )\n",
        "\n",
        "vocab_size = len( the_chars )      ## 65\n",
        "\n",
        "print(  len(the_chars)  )\n",
        "\n",
        "print(  ''.join(the_chars)  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "qxIJ49kncjhV"
      },
      "outputs": [],
      "source": [
        "stoi = { ch:i for i, ch in enumerate(the_chars) }\n",
        "itos = { i:ch for i, ch in enumerate(the_chars) }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MO6fjZ-4cnGQ",
        "outputId": "70e75309-4357-4932-d391-a7b2f365e819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0, ' ': 1, '\"': 2, '$': 3, '%': 4, '&': 5, ',': 6, '-': 7, '.': 8, '/': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ':': 20, ';': 21, '?': 22, 'A': 23, 'B': 24, 'C': 25, 'D': 26, 'E': 27, 'F': 28, 'G': 29, 'H': 30, 'I': 31, 'J': 32, 'K': 33, 'L': 34, 'M': 35, 'N': 36, 'O': 37, 'P': 38, 'Q': 39, 'R': 40, 'S': 41, 'T': 42, 'U': 43, 'V': 44, 'W': 45, 'Y': 46, 'a': 47, 'b': 48, 'c': 49, 'd': 50, 'e': 51, 'f': 52, 'g': 53, 'h': 54, 'i': 55, 'j': 56, 'k': 57, 'l': 58, 'm': 59, 'n': 60, 'o': 61, 'p': 62, 'q': 63, 'r': 64, 's': 65, 't': 66, 'u': 67, 'v': 68, 'w': 69, 'x': 70, 'y': 71, 'z': 72, '\\xa0': 73, 'è': 74, '–': 75, '—': 76, '‘': 77, '’': 78}\n",
            "{0: '\\n', 1: ' ', 2: '\"', 3: '$', 4: '%', 5: '&', 6: ',', 7: '-', 8: '.', 9: '/', 10: '0', 11: '1', 12: '2', 13: '3', 14: '4', 15: '5', 16: '6', 17: '7', 18: '8', 19: '9', 20: ':', 21: ';', 22: '?', 23: 'A', 24: 'B', 25: 'C', 26: 'D', 27: 'E', 28: 'F', 29: 'G', 30: 'H', 31: 'I', 32: 'J', 33: 'K', 34: 'L', 35: 'M', 36: 'N', 37: 'O', 38: 'P', 39: 'Q', 40: 'R', 41: 'S', 42: 'T', 43: 'U', 44: 'V', 45: 'W', 46: 'Y', 47: 'a', 48: 'b', 49: 'c', 50: 'd', 51: 'e', 52: 'f', 53: 'g', 54: 'h', 55: 'i', 56: 'j', 57: 'k', 58: 'l', 59: 'm', 60: 'n', 61: 'o', 62: 'p', 63: 'q', 64: 'r', 65: 's', 66: 't', 67: 'u', 68: 'v', 69: 'w', 70: 'x', 71: 'y', 72: 'z', 73: '\\xa0', 74: 'è', 75: '–', 76: '—', 77: '‘', 78: '’'}\n"
          ]
        }
      ],
      "source": [
        "print( stoi )\n",
        "print( itos )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aXShiO6xcpPN",
        "outputId": "cce4d932-763c-47eb-bc96-a48b169ec8bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[48, 47, 54, 54]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "encode = lambda s: [ stoi[c]          for c in s   ]\n",
        "\n",
        "encode(\"bahh\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LwuikmIycrsd",
        "outputId": "613fddfe-aef6-46f3-d2c2-07f2bca3223b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'RQYY'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "decode = lambda l: ''.join(   itos[i] for i in l   )\n",
        "\n",
        "decode([40, 39, 46, 46])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "A9Nb41GfcvOx",
        "outputId": "eaa0886d-be9d-4807-88df-84d8dd0dab4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([66, 55, 66,  ..., 14,  2,  0])\n"
          ]
        }
      ],
      "source": [
        "data = torch.tensor(   encode(text), dtype=torch.long   )\n",
        "\n",
        "print( data )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "AFRgdSAAcxqy"
      },
      "outputs": [],
      "source": [
        "n          = int(   0.9*len(data)   )\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data   = data[n:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "hEM7qRkDcz25"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    if split == \"train\":\n",
        "        data = train_data\n",
        "    else:\n",
        "        data = val_data\n",
        "\n",
        "    ix = torch.randint(   len(data) - block_size, (batch_size,)   )\n",
        "\n",
        "    x  = torch.stack(    [  data[   i : i+block_size ]     for i in ix ]    )\n",
        "    y  = torch.stack(    [  data[ i+1 : i+1+block_size ]   for i in ix ]    )\n",
        "\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Xnn9dLR-c2Fl",
        "outputId": "c91df8e3-816e-47a8-90b8-90bc40ac6722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  268, 36154, 31199, 37681])\n",
            "tensor(66)\n",
            "tensor(12)\n",
            "tensor(66)\n",
            "tensor(50)\n"
          ]
        }
      ],
      "source": [
        "temp_batch_size = 4\n",
        "temp_block_size = 16\n",
        "\n",
        "## select random starting points for the 4 sentences\n",
        "ix = torch.randint(\n",
        "            len(data) - block_size,\n",
        "            (temp_batch_size,)\n",
        ")\n",
        "\n",
        "print( ix )\n",
        "\n",
        "\n",
        "\n",
        "for index_temp in ix:\n",
        "    print(  data[index_temp]  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CbLD2QqRc22R",
        "outputId": "5f196a27-0738-47d9-e1b3-ecf73bf36921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[66, 61,  1, 48, 51,  1, 52, 51, 47, 66, 67, 64, 51, 50,  1, 55],\n",
            "        [12, 10, 12, 15,  7, 54, 51, 47, 58, 66, 54,  7, 62, 58, 47, 60],\n",
            "        [66, 61, 62,  7, 15,  7, 52, 64, 61, 59,  7, 62, 67, 64, 50, 67],\n",
            "        [50, 55, 65, 49, 55, 62, 58, 55, 60, 47, 64, 71,  1, 47, 62, 62]])\n",
            "tensor([[61,  1, 48, 51,  1, 52, 51, 47, 66, 67, 64, 51, 50,  1, 55, 60],\n",
            "        [10, 12, 15,  7, 54, 51, 47, 58, 66, 54,  7, 62, 58, 47, 60, 65],\n",
            "        [61, 62,  7, 15,  7, 52, 64, 61, 59,  7, 62, 67, 64, 50, 67, 51],\n",
            "        [55, 65, 49, 55, 62, 58, 55, 60, 47, 64, 71,  1, 47, 62, 62, 64]])\n"
          ]
        }
      ],
      "source": [
        "x  = torch.stack(\n",
        "    [ data[   i : i+  temp_block_size ]   for i in ix ]\n",
        "\n",
        ")\n",
        "\n",
        "y  = torch.stack(\n",
        "    [ data[ i+1 : i+1+ temp_block_size ]  for i in ix ]\n",
        ")\n",
        "\n",
        "print(x)\n",
        "print(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "oJ-VBD8Ac5je"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()    ## for efficient processing\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()   ## set to no training\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()  ## back to training\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "EM7FDQ54c7dX"
      },
      "outputs": [],
      "source": [
        "## NN Architectures\n",
        "\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.key   = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)  ## [512, 64]\n",
        "\n",
        "        tril_def = torch.tril( torch.ones(block_size, block_size) )  ## [40, 40]\n",
        "\n",
        "        self.register_buffer(\n",
        "                  'tril',\n",
        "                  tril_def\n",
        "               )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, E = x.shape   ## [batch_size, 40, 512]\n",
        "\n",
        "        k = self.key(   x )            ## k = (B, T, 64)\n",
        "        q = self.query( x )            ## q = (B, T, 64)\n",
        "\n",
        "        E2 = 64     ## I think this is 64 and not 512\n",
        "        ## (B, T, E) @ (B, E, T)  -> (B, T, T)\n",
        "        wei = q @ k.transpose(-2, -1) * E2 ** -0.5\n",
        "\n",
        "        wei = wei.masked_fill(\n",
        "                      self.tril[:T, :T] == 0,\n",
        "                      float('-inf')\n",
        "        )\n",
        "\n",
        "        ## (B, T, T)\n",
        "        wei = F.softmax( wei, dim= -1 )         ## (B, T, T)\n",
        "        wei = self.dropout(   wei   )\n",
        "\n",
        "        ## perform weighted aggregation of values\n",
        "\n",
        "        v   = self.value(  x  )   ## x = (B, 40, E)\n",
        "        out = wei @ v             ## (B, T, T) @ (B, T, 64) -> (B, T, 64)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "YasoTMJJc_tK"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):         ## 512\n",
        "\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),      ## [512, 4*512]\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),      ## [4*512, 512]\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "rbMqEu17dDNr"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):    ## (8, 64)\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(  [ Head(head_size) for _ in range(num_heads) ] )\n",
        "        self.proj  = nn.Linear(n_embd, n_embd)   ## 512, 512\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat(   [ h(x) for h in self.heads ], dim = -1   )\n",
        "        out = self.proj(  out   )\n",
        "        out = self.dropout(   out   )\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "oOMYLbUXdGnK"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):     ## (512, 8)\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head        ## 64\n",
        "        self.sa   = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward( n_embd)    ## 512\n",
        "        self.ln1  = nn.LayerNorm(n_embd)\n",
        "        self.ln2  = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(     self.ln1(x)      )\n",
        "        x = x + self.ffwd(   self.ln2(x)      )\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "2LutHZyvdLFa"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)   ## [65, 512]\n",
        "        self.pos_emb_table = nn.Embedding(block_size, n_embd)     ## [block, 512]\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "                *[   Block(n_embd, n_head=n_head) for _ in range(n_layer)    ]\n",
        "        )\n",
        "\n",
        "        self.ln_f    = nn.LayerNorm(  n_embd    )\n",
        "        self.lm_ffw_head = nn.Linear(n_embd, vocab_size)  ## [512, 65] # FFW Layer\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape     ## (Batch, 40)\n",
        "        ## ids and targets are both (B, T) tensors of integers\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.pos_emb_table(torch.arange(T, device=device))\n",
        "\n",
        "        x = tok_emb + pos_emb    ## [B, T, E] or [64, 40, 512]\n",
        "\n",
        "        ## This is the architecture\n",
        "        x = self.blocks(  x  )   ## (B, T, E)\n",
        "        x = self.ln_f(    x  )   ## (B, T, E)   ## norm\n",
        "        logits = self.lm_ffw_head(x)         ## [B, 40, 65]\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, E  = logits.shape\n",
        "            logits  = logits.view( B*T, E)\n",
        "            targets = targets.view(B*T)\n",
        "            loss    = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):    ## idx is (B, T)\n",
        "        for _ in range(max_new_tokens):\n",
        "            ## crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)    ## ## get preds\n",
        "            logits = logits[:, -1, :]    ## focus on last one (B, E)\n",
        "            probs = F.softmax(logits, dim= -1)    ## (B, E) get probs\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)     ## (B, 1) selected\n",
        "            idx = torch.cat(  (idx, idx_next), dim=1  )   ## (B, T+1) append sample to running sequence\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "rzmFothGdOuQ"
      },
      "outputs": [],
      "source": [
        "model   = GPTModel()\n",
        "\n",
        "m       = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(  m.parameters(), lr=learning_rate   )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rGHYc1cAdRDn",
        "outputId": "9adda61b-8f69-426a-e55f-b8b5cdeae7e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6155, val loss 4.6082\n",
            "step 500: train loss 0.9434, val loss 1.4807\n",
            "step 1000: train loss 0.5123, val loss 1.6217\n",
            "step 1500: train loss 0.3491, val loss 1.8562\n",
            "step 2000: train loss 0.2954, val loss 2.0876\n",
            "step 2500: train loss 0.2712, val loss 2.2831\n",
            "step 3000: train loss 0.2565, val loss 2.3121\n",
            "step 3500: train loss 0.2474, val loss 2.4133\n",
            "step 4000: train loss 0.2399, val loss 2.4684\n",
            "step 4500: train loss 0.2344, val loss 2.5626\n",
            "step 5000: train loss 0.2315, val loss 2.5907\n",
            "step 5500: train loss 0.2281, val loss 2.6477\n"
          ]
        }
      ],
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    ## eval the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)   ## zero out\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "756721b0",
        "outputId": "cfcc2612-4165-445d-8d98-c95e9f632d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1483 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " NASA's Mars 2020 rover is only weeks away from emerging from the 28-mile-wide Jezero Crater to explore new terrain . The crater rim is like the edge of the world, and it feels like we’re so close to going over the edge, says planetary scientist Briony Horgan .\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from newspaper import Article\n",
        "\n",
        "# Load the CSV file\n",
        "def load_csv(file_path):\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Fetch the article text using the link\n",
        "def fetch_article_text(link):\n",
        "    article = Article(link)\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    return article.text\n",
        "\n",
        "# Summarize the article\n",
        "def summarize_article(text, max_length=150):\n",
        "    from transformers import pipeline  # Import pipeline here\n",
        "    # Specify the model you want to use here\n",
        "    # You can replace this with any summarization model from Hugging Face\n",
        "    model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
        "\n",
        "    # Use GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    summarizer = pipeline(\"summarization\", model=model_name, device=device)\n",
        "\n",
        "    # Truncate the text if it's too long\n",
        "    max_input_length = summarizer.model.config.max_position_embeddings  # Get max input length\n",
        "    if len(summarizer.tokenizer.encode(text)) > max_input_length:\n",
        "        text = text[:max_input_length]  # Truncate to max length\n",
        "\n",
        "    summary = summarizer(text, max_length=max_length, min_length=30, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "# Search and summarize by title\n",
        "def get_summary_by_title(csv_file_path, title):\n",
        "    # Load data\n",
        "    df = load_csv(csv_file_path)\n",
        "    # Search for the title\n",
        "    article_info = df[df['title'] == title]\n",
        "    if article_info.empty:\n",
        "        return \"Title not found in the dataset.\"\n",
        "\n",
        "    link = article_info.iloc[0]['link']\n",
        "    # Fetch and summarize the article\n",
        "    try:\n",
        "        article_text = fetch_article_text(link)\n",
        "        summary = summarize_article(article_text)\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching or summarizing article: {str(e)}\"\n",
        "\n",
        "# Example usage\n",
        "csv_file_path = \"/content/purdue_news_all_pages-2024.csv\"\n",
        "title = \"Purdue scientist expecting new world to reveal itself to Mars rover\"\n",
        "print(get_summary_by_title(csv_file_path, title))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml_html_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NjQHX3EPgZUE",
        "outputId": "4e924ea1-a6de-4a2c-d95c-99dff2ad4940"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from lxml_html_clean) (5.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k # Install the missing library with pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IySjH8wjgOyS",
        "outputId": "9e4c00b7-47ef-4609-dfe7-cd52586b0364"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (11.0.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (5.3.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.32.3)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (5.1.3)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2024.8.30)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.16.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from newspaper import Article\n",
        "from transformers import pipeline  # Import pipeline at the top\n",
        "\n",
        "# Load the CSV file\n",
        "def load_csv(file_path):\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Fetch the article text using the link\n",
        "def fetch_article_text(link):\n",
        "    article = Article(link)\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    return article.text\n",
        "\n",
        "# Summarize the article\n",
        "def summarize_article(text, word_count=100):\n",
        "    # Specify the model you want to use here\n",
        "    # You can replace this with any summarization model from Hugging Face\n",
        "    model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
        "\n",
        "    # Use GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    summarizer = pipeline(\"summarization\", model=model_name, device=device)\n",
        "\n",
        "    # Estimate max_length based on word_count\n",
        "    # Assuming ~1.3 tokens per word\n",
        "    token_count = int(word_count * 1.3)\n",
        "    max_length = token_count\n",
        "    min_length = int(token_count * 0.8)  # Ensure some flexibility for shorter summaries\n",
        "\n",
        "    # Truncate the text if it's too long\n",
        "    max_input_length = summarizer.model.config.max_position_embeddings  # Get max input length\n",
        "    if len(summarizer.tokenizer.encode(text)) > max_input_length:\n",
        "        text = text[:max_input_length]  # Truncate to max length\n",
        "\n",
        "    summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "# Search and summarize by title\n",
        "def get_summary_by_title(csv_file_path, title):\n",
        "    # Load data\n",
        "    df = load_csv(csv_file_path)\n",
        "    # Search for the title\n",
        "    article_info = df[df['title'] == title]\n",
        "    if article_info.empty:\n",
        "        return \"Title not found in the dataset.\"\n",
        "\n",
        "    link = article_info.iloc[0]['link']\n",
        "    # Fetch and summarize the article\n",
        "    try:\n",
        "        article_text = fetch_article_text(link)\n",
        "        summary = summarize_article(article_text, word_count=100)\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching or summarizing article: {str(e)}\"\n",
        "\n",
        "# Main program\n",
        "csv_file_path = \"/content/purdue_news_all_pages-2024.csv\"\n",
        "print(\"Enter the title of the article you want to summarize:\")\n",
        "title = input(\"> \")  # Taking user input for the title\n",
        "summary = get_summary_by_title(csv_file_path, title)\n",
        "print(\"\\nSummary:\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5UdM5QMOhUnI",
        "outputId": "b64738e1-b2dd-4946-b49a-d1453b44ae69"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the title of the article you want to summarize:\n",
            "> Participants in the Purdue Ukrainian Scholars Initiative to be featured in annual panel discussion\n",
            "\n",
            "Summary:\n",
            " The hourlong event will be in the East Faculty Lounge, on the Purdue Memorial Union’s second floor . The discussion will include a brief overview of the Purdue Ukrainian Scholars Initiative . The program is one of the first and largest of similar university-led programs in the United States . This year Purdue hosts 15 scholars, who are continuing their research here . The event is free and open to the public (registration required), and will be on Tuesday (Dec. 10) at 4:30 p.m. The discussion is led by Vijay Raghunathan, vice president for global partnerships .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from newspaper import Article\n",
        "from transformers import pipeline  # Import pipeline at the top\n",
        "from googletrans import Translator\n",
        "\n",
        "# Load the CSV file\n",
        "def load_csv(file_path):\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Fetch the article text using the link\n",
        "def fetch_article_text(link):\n",
        "    article = Article(link)\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    return article.text\n",
        "\n",
        "# Summarize the article\n",
        "def summarize_article(text, word_count=100):\n",
        "    model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
        "\n",
        "    # Use GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    summarizer = pipeline(\"summarization\", model=model_name, device=device)\n",
        "\n",
        "    # Estimate max_length based on word_count\n",
        "    token_count = int(word_count * 1.3)\n",
        "    max_length = token_count\n",
        "    min_length = int(token_count * 0.8)\n",
        "\n",
        "    # Truncate the text if it's too long\n",
        "    max_input_length = summarizer.model.config.max_position_embeddings\n",
        "    if len(summarizer.tokenizer.encode(text)) > max_input_length:\n",
        "        text = text[:max_input_length]\n",
        "\n",
        "    summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "# Search and summarize by title\n",
        "def get_summary_by_title(csv_file_path, title):\n",
        "    # Load data\n",
        "    df = load_csv(csv_file_path)\n",
        "\n",
        "    # Search for the title\n",
        "    article_info = df[df['title'] == title]\n",
        "    if article_info.empty:\n",
        "        return \"Title not found in the dataset.\"\n",
        "\n",
        "    link = article_info.iloc[0]['link']\n",
        "\n",
        "    # Fetch and summarize the article\n",
        "    try:\n",
        "        article_text = fetch_article_text(link)\n",
        "        summary = summarize_article(article_text, word_count=100)\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching or summarizing article: {str(e)}\"\n",
        "\n",
        "# Translate text\n",
        "def translate_text(text, src_lang='en', dest_lang='es'):\n",
        "    translator = Translator()\n",
        "    translation = translator.translate(text, src=src_lang, dest=dest_lang)\n",
        "    return translation.text\n",
        "\n",
        "# Main program\n",
        "csv_file_path = \"/content/purdue_news_all_pages-2024.csv\"\n",
        "print(\"Enter the title of the article you want to summarize:\")\n",
        "title = input(\"> \")  # Taking user input for the title\n",
        "summary = get_summary_by_title(csv_file_path, title)\n",
        "\n",
        "if \"Error\" not in summary and \"not found\" not in summary:\n",
        "    print(\"\\nSummary:\")\n",
        "    print(summary)\n",
        "\n",
        "    # Translate the summary\n",
        "    translated_summary = translate_text(summary, src_lang='en', dest_lang='es')\n",
        "    print(\"\\nTranslated Summary (Spanish):\")\n",
        "    print(translated_summary)\n",
        "else:\n",
        "    print(\"\\n\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NYgkvtap6kFo",
        "outputId": "d5127dec-af09-4370-bcaf-8a02a8ac8331"
      },
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the title of the article you want to summarize:\n",
            "> Purdue scientist expecting new world to reveal itself to Mars rover\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1483 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary:\n",
            " NASA's Mars 2020 rover is only weeks away from emerging from the 28-mile-wide Jezero Crater to explore new terrain . The crater rim is like the edge of the world, and it feels like we’re so close to going over the edge, says Purdue University planetary scientist Briony Horgan . Horgan anticipates some of the oldest rocks yet as NASA mission prepares to emerge from the crater . It's a point in the mission Horgan set her sights on following the rover’s landing four years ago .\n",
            "\n",
            "Translated Summary (Spanish):\n",
            "El Rover 2020 de la NASA está a solo unas semanas de emerger del cráter Jezero de 28 millas de ancho para explorar un nuevo terreno.El borde del cráter es como el borde del mundo, y parece que estamos tan cerca de ir al límite, dice el científico planetario de la Universidad de Purdue, Briony Horgan.Horgan anticipa algunas de las rocas más antiguas hasta ahora mientras la misión de la NASA se prepara para emerger del cráter.Es un punto en la Misión de la Misión, Horgan, se dedica a seguir el aterrizaje del rover hace cuatro años.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1362
        },
        "id": "rjL0Leur7aU3",
        "outputId": "c61c0dad-f329-42f6-e0cd-cdc83e79876e"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.12.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2024.12.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=509c33c3f5bd10eb934f1f8f66c7e4a953315d0ec8160040cf12f88a844b1383\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.2.2 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.54.5 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chardet",
                  "idna"
                ]
              },
              "id": "d8aefb33f91b4396b01b99899d685610"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "def load_csv(file_path):\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Fetch the main image from the URL\n",
        "def fetch_image_from_url(article_url):\n",
        "    try:\n",
        "        response = requests.get(article_url, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            # Look for 'og:image' meta tag\n",
        "            image_url = soup.find(\"meta\", property=\"og:image\")\n",
        "            if image_url and image_url[\"content\"]:\n",
        "                return image_url[\"content\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching image URL from {article_url}: {e}\")\n",
        "    return None\n",
        "\n",
        "# Fetch and process the image\n",
        "def fetch_and_describe_image(image_url):\n",
        "    try:\n",
        "        response = requests.get(image_url, stream=True, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            image = Image.open(io.BytesIO(response.content))\n",
        "\n",
        "            # Load BLIP model for generating image descriptions\n",
        "            processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "            model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "            # Preprocess image\n",
        "            inputs = processor(image, return_tensors=\"pt\")\n",
        "\n",
        "            # Generate caption\n",
        "            caption = model.generate(**inputs)\n",
        "            return processor.decode(caption[0], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image: {e}\")\n",
        "    return \"No description available\"\n",
        "\n",
        "# Fetch the link from the dataset and generate the description\n",
        "def get_image_and_description_by_title(csv_file_path, title):\n",
        "    # Load data\n",
        "    df = load_csv(csv_file_path)\n",
        "\n",
        "    # Search for the title\n",
        "    article_info = df[df['title'] == title]\n",
        "    if article_info.empty:\n",
        "        return \"Title not found in the dataset.\"\n",
        "\n",
        "    link = article_info.iloc[0]['link']\n",
        "    print(f\"Fetched link: {link}\")\n",
        "\n",
        "    # Fetch the image and generate description\n",
        "    try:\n",
        "        image_url = fetch_image_from_url(link)\n",
        "        if image_url:\n",
        "            print(f\"Fetched image URL: {image_url}\")\n",
        "            description = fetch_and_describe_image(image_url)\n",
        "            return {\n",
        "                \"image_url\": image_url,\n",
        "                \"description\": description\n",
        "            }\n",
        "        else:\n",
        "            return \"No image found for this article.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error processing article: {str(e)}\"\n",
        "\n",
        "# Example usage\n",
        "csv_file_path = \"/content/purdue_news_all_pages-2024.csv\"\n",
        "title = input(\"Enter the title of the article: \")\n",
        "result = get_image_and_description_by_title(csv_file_path, title)\n",
        "\n",
        "# Output the result\n",
        "if isinstance(result, dict):\n",
        "    print(f\"Image URL: {result['image_url']}\")\n",
        "    print(f\"Description: {result['description']}\")\n",
        "else:\n",
        "    print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gDeOOvxAvNUr",
        "outputId": "73c6ce72-919e-4add-c930-d4eec82350df"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the title of the article: Purdue scientist expecting new world to reveal itself to Mars rover\n",
            "Fetched link: https://www.purdue.edu/newsroom/2024/Q4/purdue-scientist-expecting-new-world-to-reveal-itself-to-mars-rover\n",
            "Fetched image URL: https://www.purdue.edu/newsroom/wp-content/uploads/2024/12/BrionyHorgan-PerseveranceOG-scaled.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image URL: https://www.purdue.edu/newsroom/wp-content/uploads/2024/12/BrionyHorgan-PerseveranceOG-scaled.jpg\n",
            "Description: a woman standing next to a large object\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
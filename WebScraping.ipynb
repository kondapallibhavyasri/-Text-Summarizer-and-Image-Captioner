{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wh_4cickhq1",
        "outputId": "01240f9e-2456-4543-bfb9-40252cb01cbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# URL of the Purdue News articles page\n",
        "url = \"https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&paged=1&filter_year=2024&custom_post_type=post,purduetoday\"\n",
        "\n",
        "# Step 1: Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    print(\"Website successfully accessed!\")\n",
        "else:\n",
        "    print(f\"Failed to access the website. Status code: {response.status_code}\")\n",
        "\n",
        "# Step 2: Parse the HTML content of the page\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Step 3: Extract article titles, links, and dates\n",
        "articles = soup.find_all('a', class_='purdue-home-cta-card purdue-home-cta-card--story post-type-article')\n",
        "\n",
        "news_data = []\n",
        "for article in articles:\n",
        "    # Extract the title\n",
        "    title_tag = article.find('p', class_='purdue-home-cta-grid__card-title')\n",
        "    title = title_tag.text.strip() if title_tag else \"No Title\"\n",
        "\n",
        "    # Extract the link\n",
        "    link = article['href']\n",
        "\n",
        "    # Extract the date\n",
        "    date_tag = article.find('span', class_='purdue-date-tag')\n",
        "    date = date_tag.text.strip() if date_tag else \"No Date\"\n",
        "\n",
        "    # Append the data to the list\n",
        "    news_data.append({'title': title, 'link': link, 'date': date})\n",
        "\n",
        "# Check if data was extracted\n",
        "if news_data:\n",
        "    print(f\"\\nExtracted {len(news_data)} articles:\")\n",
        "    for i, news in enumerate(news_data, start=1):\n",
        "        print(f\"{i}. Title: {news['title']}\")\n",
        "        print(f\"   Link: {news['link']}\")\n",
        "        print(f\"   Date: {news['date']}\")\n",
        "else:\n",
        "    print(\"\\nNo articles found. Check the website structure and update the selector.\")\n",
        "\n",
        "# Step 4: Save data to a CSV file\n",
        "with open(\"purdue_news_updated.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=[\"title\", \"link\", \"date\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(news_data)\n",
        "\n",
        "print(\"\\nData saved to 'purdue_news_updated.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0aD67DxksW6",
        "outputId": "650e04cd-b591-4417-8975-b6a367d04469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Website successfully accessed!\n",
            "\n",
            "Extracted 4 articles:\n",
            "1. Title: Purdue scientist expecting new world to reveal itself to Mars rover\n",
            "   Link: https://www.purdue.edu/newsroom/2024/Q4/purdue-scientist-expecting-new-world-to-reveal-itself-to-mars-rover\n",
            "   Date: December 3, 2024\n",
            "2. Title: Participants in the Purdue Ukrainian Scholars Initiative to be featured in annual panel discussion\n",
            "   Link: https://www.purdue.edu/newsroom/2024/Q4/participants-in-the-purdue-ukrainian-scholars-initiative-to-be-featured-in-annual-panel-discussion\n",
            "   Date: December 3, 2024\n",
            "3. Title: Cement grows stronger, more resilient with Purdue cellulose innovation\n",
            "   Link: https://www.purdue.edu/newsroom/2024/Q4/cement-grows-stronger-more-resilient-with-purdue-cellulose-innovation\n",
            "   Date: December 3, 2024\n",
            "4. Title: Luna Lu appointed vice president of Office of Industry Partnerships\n",
            "   Link: https://www.purdue.edu/newsroom/2024/Q4/luna-lu-appointed-vice-president-of-office-of-industry-partnerships\n",
            "   Date: November 25, 2024\n",
            "\n",
            "Data saved to 'purdue_news_updated.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Base URL for Purdue News articles\n",
        "base_url = \"https://www.purdue.edu/newsroom/articles/\"\n",
        "\n",
        "# Initialize an empty list to store all articles\n",
        "all_news_data = []\n",
        "\n",
        "# Start pagination\n",
        "page = 1\n",
        "while True:\n",
        "    # Construct the URL for the current page\n",
        "    url = f\"{base_url}?order=DESC&orderby=date&paged={page}&filter_year=2024&custom_post_type=post,purduetoday\"\n",
        "    print(f\"Scraping page {page}...\")\n",
        "\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to access page {page}. Status code: {response.status_code}\")\n",
        "        break\n",
        "\n",
        "    # Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all article blocks on the page\n",
        "    articles = soup.find_all('a', class_='purdue-home-cta-card purdue-home-cta-card--story post-type-article')\n",
        "\n",
        "    # If no articles are found, stop the loop\n",
        "    if not articles:\n",
        "        print(\"No more articles found. Stopping pagination.\")\n",
        "        break\n",
        "\n",
        "    # Extract data from each article\n",
        "    for article in articles:\n",
        "        title_tag = article.find('p', class_='purdue-home-cta-grid__card-title')\n",
        "        title = title_tag.text.strip() if title_tag else \"No Title\"\n",
        "\n",
        "        link = article['href']\n",
        "\n",
        "        date_tag = article.find('span', class_='purdue-date-tag')\n",
        "        date = date_tag.text.strip() if date_tag else \"No Date\"\n",
        "\n",
        "        all_news_data.append({'title': title, 'link': link, 'date': date})\n",
        "\n",
        "    # Move to the next page\n",
        "    page += 1\n",
        "\n",
        "# Save all collected data to a CSV file\n",
        "with open(\"purdue_news_all_pages.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=[\"title\", \"link\", \"date\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(all_news_data)\n",
        "\n",
        "print(f\"\\nScraped {len(all_news_data)} articles in total.\")\n",
        "print(\"Data saved to 'purdue_news_all_pages.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQvoFYLWqB48",
        "outputId": "31e46f24-bf5e-4381-ea8c-eb02889fd4a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Failed to access page 2. Status code: 500\n",
            "\n",
            "Scraped 4 articles in total.\n",
            "Data saved to 'purdue_news_all_pages.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Base URL for Purdue News articles\n",
        "base_url = \"https://www.purdue.edu/newsroom/articles/\"\n",
        "\n",
        "# Initialize an empty list to store all articles\n",
        "all_news_data = []\n",
        "\n",
        "# Start pagination\n",
        "page = 1\n",
        "while True:\n",
        "    # Construct the URL for the current page\n",
        "    url = f\"{base_url}?order=DESC&orderby=date&paged={page}&filter_year=2024&custom_post_type=post,purduetoday\"\n",
        "    print(f\"Scraping page {page}...\")\n",
        "\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to access page {page}. Status code: {response.status_code}\")\n",
        "        break\n",
        "\n",
        "    # Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all article blocks on the page\n",
        "    articles = soup.find_all('a', class_='purdue-home-cta-card purdue-home-cta-card--story post-type-article')\n",
        "\n",
        "    # If no articles are found, stop the loop\n",
        "    if not articles:\n",
        "        print(\"No more articles found. Stopping pagination.\")\n",
        "        break\n",
        "\n",
        "    # Extract data from each article\n",
        "    for article in articles:\n",
        "        title_tag = article.find('p', class_='purdue-home-cta-grid__card-title')\n",
        "        title = title_tag.text.strip() if title_tag else \"No Title\"\n",
        "\n",
        "        link = article['href']\n",
        "\n",
        "        date_tag = article.find('span', class_='purdue-date-tag')\n",
        "        date = date_tag.text.strip() if date_tag else \"No Date\"\n",
        "\n",
        "        all_news_data.append({'title': title, 'link': link, 'date': date})\n",
        "\n",
        "    # Move to the next page\n",
        "    page += 1\n",
        "\n",
        "# Save all collected data to a CSV file\n",
        "with open(\"purdue_news_all_pages.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=[\"title\", \"link\", \"date\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(all_news_data)\n",
        "\n",
        "print(f\"\\nScraped {len(all_news_data)} articles in total.\")\n",
        "print(\"Data saved to 'purdue_news_all_pages.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCvxZovk9goq",
        "outputId": "05849a79-5caf-4d2b-8f6d-910f23f17574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Scraping page 4...\n",
            "Scraping page 5...\n",
            "Scraping page 6...\n",
            "Scraping page 7...\n",
            "Scraping page 8...\n",
            "Scraping page 9...\n",
            "Scraping page 10...\n",
            "Scraping page 11...\n",
            "Scraping page 12...\n",
            "Failed to access page 12. Status code: 500\n",
            "\n",
            "Scraped 45 articles in total.\n",
            "Data saved to 'purdue_news_all_pages.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Base URL for Purdue News articles\n",
        "base_url = \"https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&paged=1&filter_year=2022&custom_post_type=post,purduetoday\"\n",
        "\n",
        "# User-Agent to mimic a real browser\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Function to fetch a URL with retry logic\n",
        "def fetch_url(url, retries=3, delay=5):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                return response\n",
        "            print(f\"Attempt {attempt + 1} failed. Status code: {response.status_code}. Retrying in {delay} seconds...\")\n",
        "            time.sleep(delay)\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Attempt {attempt + 1} failed with error: {e}. Retrying in {delay} seconds...\")\n",
        "            time.sleep(delay)\n",
        "    print(f\"Failed to fetch {url} after {retries} attempts.\")\n",
        "    return None\n",
        "\n",
        "# Initialize an empty list to store all articles\n",
        "all_news_data = []\n",
        "\n",
        "# Start pagination\n",
        "page = 1\n",
        "while True:\n",
        "    # Construct the URL for the current page\n",
        "    url = f\"{base_url}?order=DESC&orderby=date&paged={page}&filter_year=2024&custom_post_type=post,purduetoday\"\n",
        "    print(f\"Scraping page {page}...\")\n",
        "\n",
        "    # Fetch the page content\n",
        "    response = fetch_url(url)\n",
        "    if not response:\n",
        "        print(f\"Stopping pagination at page {page} due to repeated failures.\")\n",
        "        break\n",
        "\n",
        "    # Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all article blocks on the page\n",
        "    articles = soup.find_all('a', class_='purdue-home-cta-card purdue-home-cta-card--story post-type-article')\n",
        "\n",
        "    # If no articles are found, stop the loop\n",
        "    if not articles:\n",
        "        print(\"No more articles found. Stopping pagination.\")\n",
        "        break\n",
        "\n",
        "    # Extract data from each article\n",
        "    for article in articles:\n",
        "        title_tag = article.find('p', class_='purdue-home-cta-grid__card-title')\n",
        "        title = title_tag.text.strip() if title_tag else \"No Title\"\n",
        "\n",
        "        link = article['href']\n",
        "\n",
        "        date_tag = article.find('span', class_='purdue-date-tag')\n",
        "        date = date_tag.text.strip() if date_tag else \"No Date\"\n",
        "\n",
        "        all_news_data.append({'title': title, 'link': link, 'date': date})\n",
        "\n",
        "    # Move to the next page\n",
        "    page += 1\n",
        "\n",
        "    # Add a delay to prevent overloading the server\n",
        "    time.sleep(2)\n",
        "\n",
        "# Save all collected data to a CSV file\n",
        "with open(\"purdue_news_all_pages.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=[\"title\", \"link\", \"date\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(all_news_data)\n",
        "\n",
        "print(f\"\\nScraped {len(all_news_data)} articles in total.\")\n",
        "print(\"Data saved to 'purdue_news_all_pages-2022.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVuOoWczBbG8",
        "outputId": "c25afcf5-cfac-4b57-a3ff-2a7e9c059d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Attempt 1 failed. Status code: 500. Retrying in 5 seconds...\n",
            "Scraping page 4...\n",
            "Attempt 1 failed. Status code: 500. Retrying in 5 seconds...\n",
            "Scraping page 5...\n",
            "Scraping page 6...\n",
            "Scraping page 7...\n",
            "Scraping page 8...\n",
            "Scraping page 9...\n",
            "Scraping page 10...\n",
            "Scraping page 11...\n",
            "Scraping page 12...\n",
            "Scraping page 13...\n",
            "Scraping page 14...\n",
            "Scraping page 15...\n",
            "Scraping page 16...\n",
            "Scraping page 17...\n",
            "Scraping page 18...\n",
            "Scraping page 19...\n",
            "Scraping page 20...\n",
            "Scraping page 21...\n",
            "Scraping page 22...\n",
            "Scraping page 23...\n",
            "Scraping page 24...\n",
            "Scraping page 25...\n",
            "Scraping page 26...\n",
            "Scraping page 27...\n",
            "Scraping page 28...\n",
            "Attempt 1 failed. Status code: 500. Retrying in 5 seconds...\n",
            "Scraping page 29...\n",
            "Scraping page 30...\n",
            "Scraping page 31...\n",
            "Scraping page 32...\n",
            "Scraping page 33...\n",
            "Scraping page 34...\n",
            "Scraping page 35...\n",
            "Scraping page 36...\n",
            "Scraping page 37...\n",
            "Scraping page 38...\n",
            "Scraping page 39...\n",
            "Scraping page 40...\n",
            "Scraping page 41...\n",
            "Scraping page 42...\n",
            "Scraping page 43...\n",
            "Scraping page 44...\n",
            "Scraping page 45...\n",
            "Scraping page 46...\n",
            "Attempt 1 failed. Status code: 500. Retrying in 5 seconds...\n",
            "Scraping page 47...\n",
            "Scraping page 48...\n",
            "Scraping page 49...\n",
            "Scraping page 50...\n",
            "Scraping page 51...\n",
            "Scraping page 52...\n",
            "Scraping page 53...\n",
            "Scraping page 54...\n",
            "Scraping page 55...\n",
            "Scraping page 56...\n",
            "Scraping page 57...\n",
            "Scraping page 58...\n",
            "Attempt 1 failed. Status code: 500. Retrying in 5 seconds...\n",
            "Scraping page 59...\n",
            "Scraping page 60...\n",
            "Scraping page 61...\n",
            "Scraping page 62...\n",
            "Scraping page 63...\n",
            "Scraping page 64...\n",
            "Scraping page 65...\n",
            "Scraping page 66...\n",
            "Scraping page 67...\n",
            "Attempt 1 failed. Status code: 500. Retrying in 5 seconds...\n",
            "No more articles found. Stopping pagination.\n",
            "\n",
            "Scraped 352 articles in total.\n",
            "Data saved to 'purdue_news_all_pages-2022.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Base URL with the necessary query parameters for pagination\n",
        "base_url = \"https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post,purduetoday\"\n",
        "\n",
        "# User-Agent to mimic a real browser\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Function to fetch a URL with retry logic\n",
        "def fetch_url(url, retries=3, delay=5):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                return response\n",
        "            print(f\"Attempt {attempt + 1} failed. Status code: {response.status_code}. Retrying in {delay} seconds...\")\n",
        "            time.sleep(delay)\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Attempt {attempt + 1} failed with error: {e}. Retrying in {delay} seconds...\")\n",
        "            time.sleep(delay)\n",
        "    print(f\"Failed to fetch {url} after {retries} attempts.\")\n",
        "    return None\n",
        "\n",
        "# Initialize an empty list to store all articles\n",
        "all_news_data = []\n",
        "\n",
        "# Start pagination\n",
        "page = 1\n",
        "while True:\n",
        "    # Construct the URL for the current page\n",
        "    url = f\"{base_url}&paged={page}\"\n",
        "    print(f\"Scraping page {page}...\")\n",
        "\n",
        "    # Fetch the page content\n",
        "    response = fetch_url(url)\n",
        "    if not response:\n",
        "        print(f\"Stopping pagination at page {page} due to repeated failures.\")\n",
        "        break\n",
        "\n",
        "    # Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all article blocks on the page\n",
        "    articles = soup.find_all('a', class_='purdue-home-cta-card purdue-home-cta-card--story post-type-article')\n",
        "\n",
        "    # If no articles are found, stop the loop\n",
        "    if not articles:\n",
        "        print(\"No more articles found. Stopping pagination.\")\n",
        "        break\n",
        "\n",
        "    # Extract data from each article\n",
        "    for article in articles:\n",
        "        title_tag = article.find('p', class_='purdue-home-cta-grid__card-title')\n",
        "        title = title_tag.text.strip() if title_tag else \"No Title\"\n",
        "\n",
        "        link = article['href']\n",
        "\n",
        "        date_tag = article.find('span', class_='purdue-date-tag')\n",
        "        date = date_tag.text.strip() if date_tag else \"No Date\"\n",
        "\n",
        "        all_news_data.append({'title': title, 'link': link, 'date': date})\n",
        "\n",
        "    # Move to the next page\n",
        "    page += 1\n",
        "\n",
        "    # Add a delay to prevent overloading the server\n",
        "    time.sleep(2)\n",
        "\n",
        "# Save all collected data to a CSV file\n",
        "with open(\"purdue_news_2022.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=[\"title\", \"link\", \"date\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(all_news_data)\n",
        "\n",
        "print(f\"\\nScraped {len(all_news_data)} articles in total.\")\n",
        "print(\"Data saved to 'purdue_news_2022.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNv6YZNgXe8E",
        "outputId": "53b6f354-ee16-4370-9dd4-24d81c00a5f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Scraping page 4...\n",
            "Scraping page 5...\n",
            "Scraping page 6...\n",
            "Scraping page 7...\n",
            "Scraping page 8...\n",
            "Scraping page 9...\n",
            "Scraping page 10...\n",
            "Scraping page 11...\n",
            "Scraping page 12...\n",
            "Scraping page 13...\n",
            "Scraping page 14...\n",
            "Scraping page 15...\n",
            "Scraping page 16...\n",
            "Scraping page 17...\n",
            "Scraping page 18...\n",
            "Scraping page 19...\n",
            "Scraping page 20...\n",
            "Scraping page 21...\n",
            "Scraping page 22...\n",
            "Scraping page 23...\n",
            "Scraping page 24...\n",
            "Scraping page 25...\n",
            "Scraping page 26...\n",
            "Scraping page 27...\n",
            "Scraping page 28...\n",
            "Scraping page 29...\n",
            "Scraping page 30...\n",
            "Scraping page 31...\n",
            "Scraping page 32...\n",
            "Scraping page 33...\n",
            "Scraping page 34...\n",
            "Scraping page 35...\n",
            "Scraping page 36...\n",
            "Attempt 1 failed. Status code: 500. Retrying in 5 seconds...\n",
            "Scraping page 37...\n",
            "Scraping page 38...\n",
            "Scraping page 39...\n",
            "Scraping page 40...\n",
            "Scraping page 41...\n",
            "Scraping page 42...\n",
            "Scraping page 43...\n",
            "Scraping page 44...\n",
            "Scraping page 45...\n",
            "Scraping page 46...\n",
            "Scraping page 47...\n",
            "Scraping page 48...\n",
            "Scraping page 49...\n",
            "Scraping page 50...\n",
            "Scraping page 51...\n",
            "Scraping page 52...\n",
            "Scraping page 53...\n",
            "Scraping page 54...\n",
            "Scraping page 55...\n",
            "Scraping page 56...\n",
            "Scraping page 57...\n",
            "Scraping page 58...\n",
            "Scraping page 59...\n",
            "Scraping page 60...\n",
            "Scraping page 61...\n",
            "Scraping page 62...\n",
            "Scraping page 63...\n",
            "Scraping page 64...\n",
            "Scraping page 65...\n",
            "Attempt 1 failed. Status code: 500. Retrying in 5 seconds...\n",
            "Scraping page 66...\n",
            "Scraping page 67...\n",
            "Attempt 1 failed. Status code: 500. Retrying in 5 seconds...\n",
            "No more articles found. Stopping pagination.\n",
            "\n",
            "Scraped 352 articles in total.\n",
            "Data saved to 'purdue_news_2022.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "from urllib.parse import urljoin, urlparse, parse_qs, urlencode\n",
        "\n",
        "# Base URL with the necessary query parameters for pagination\n",
        "base_url = \"https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post,purduetoday\"\n",
        "\n",
        "# User-Agent to mimic a real browser\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Function to fetch a URL with retry logic\n",
        "def fetch_url(url, retries=3, delay=5):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                return response\n",
        "            print(f\"Attempt {attempt + 1} failed. Status code: {response.status_code}. Retrying in {delay} seconds...\")\n",
        "            time.sleep(delay)\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Attempt {attempt + 1} failed with error: {e}. Retrying in {delay} seconds...\")\n",
        "            time.sleep(delay)\n",
        "    print(f\"Failed to fetch {url} after {retries} attempts.\")\n",
        "    return None\n",
        "\n",
        "# Parse URL and update the `paged` parameter\n",
        "def update_url_page(url, page):\n",
        "    parsed_url = urlparse(url)\n",
        "    query_params = parse_qs(parsed_url.query)\n",
        "    query_params[\"paged\"] = page  # Update page number\n",
        "    new_query_string = urlencode(query_params, doseq=True)\n",
        "    updated_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{new_query_string}\"\n",
        "    return updated_url\n",
        "\n",
        "# Initialize an empty list to store all articles\n",
        "all_news_data = []\n",
        "\n",
        "# Start pagination\n",
        "page = 1\n",
        "while True:\n",
        "    # Construct the URL for the current page\n",
        "    url = update_url_page(base_url, page)\n",
        "    print(f\"Scraping page {page}: {url}\")\n",
        "\n",
        "    # Fetch the page content\n",
        "    response = fetch_url(url)\n",
        "    if not response:\n",
        "        print(f\"Stopping pagination at page {page} due to repeated failures.\")\n",
        "        break\n",
        "\n",
        "    # Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all article blocks on the page\n",
        "    articles = soup.find_all('a', class_='purdue-home-cta-card purdue-home-cta-card--story post-type-article')\n",
        "\n",
        "    # If no articles are found, stop the loop\n",
        "    if not articles:\n",
        "        print(\"No more articles found. Stopping pagination.\")\n",
        "        break\n",
        "\n",
        "    # Extract data from each article\n",
        "    for article in articles:\n",
        "        title_tag = article.find('p', class_='purdue-home-cta-grid__card-title')\n",
        "        title = title_tag.text.strip() if title_tag else \"No Title\"\n",
        "\n",
        "        link = article['href']\n",
        "\n",
        "        date_tag = article.find('span', class_='purdue-date-tag')\n",
        "        date = date_tag.text.strip() if date_tag else \"No Date\"\n",
        "\n",
        "        all_news_data.append({'title': title, 'link': link, 'date': date})\n",
        "\n",
        "    # Move to the next page\n",
        "    page += 1\n",
        "\n",
        "    # Add a delay to prevent overloading the server\n",
        "    time.sleep(2)\n",
        "\n",
        "# Save all collected data to a CSV file\n",
        "with open(\"purdue_news_2022.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=[\"title\", \"link\", \"date\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(all_news_data)\n",
        "\n",
        "print(f\"\\nScraped {len(all_news_data)} articles in total.\")\n",
        "print(\"Data saved to 'purdue_news_2022.csv'.\")\n"
      ],
      "metadata": {
        "id": "-OZsN2zLZpL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "from urllib.parse import urljoin, urlparse, parse_qs, urlencode\n",
        "\n",
        "# Base URL with the necessary query parameters for pagination\n",
        "base_url = \"https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post,purduetoday\"\n",
        "\n",
        "# User-Agent to mimic a real browser\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Function to fetch a URL with retry logic\n",
        "def fetch_url(url, retries=3, delay=5):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                return response\n",
        "            print(f\"Attempt {attempt + 1} failed. Status code: {response.status_code}. Retrying in {delay} seconds...\")\n",
        "            time.sleep(delay)\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Attempt {attempt + 1} failed with error: {e}. Retrying in {delay} seconds...\")\n",
        "            time.sleep(delay)\n",
        "    print(f\"Failed to fetch {url} after {retries} attempts.\")\n",
        "    return None\n",
        "\n",
        "# Parse URL and update the `paged` parameter\n",
        "def update_url_page(url, page):\n",
        "    parsed_url = urlparse(url)\n",
        "    query_params = parse_qs(parsed_url.query)\n",
        "    query_params[\"paged\"] = page  # Update page number\n",
        "    new_query_string = urlencode(query_params, doseq=True)\n",
        "    updated_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{new_query_string}\"\n",
        "    return updated_url\n",
        "\n",
        "# Initialize an empty list to store all articles\n",
        "all_news_data = []\n",
        "\n",
        "# Start pagination\n",
        "page = 1\n",
        "while True:\n",
        "    # Construct the URL for the current page\n",
        "    url = update_url_page(base_url, page)\n",
        "    print(f\"Scraping page {page}: {url}\")\n",
        "\n",
        "    # Fetch the page content\n",
        "    response = fetch_url(url)\n",
        "    if not response:\n",
        "        print(f\"Stopping pagination at page {page} due to repeated failures.\")\n",
        "        break\n",
        "\n",
        "    # Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all article blocks on the page\n",
        "    articles = soup.find_all('a', class_='purdue-home-cta-card purdue-home-cta-card--story post-type-article')\n",
        "\n",
        "    # If no articles are found, stop the loop\n",
        "    if not articles:\n",
        "        print(\"No more articles found. Stopping pagination.\")\n",
        "        break\n",
        "\n",
        "    # Extract data from each article\n",
        "    for article in articles:\n",
        "        title_tag = article.find('p', class_='purdue-home-cta-grid__card-title')\n",
        "        title = title_tag.text.strip() if title_tag else \"No Title\"\n",
        "\n",
        "        link = article['href']\n",
        "\n",
        "        date_tag = article.find('span', class_='purdue-date-tag')\n",
        "        date = date_tag.text.strip() if date_tag else \"No Date\"\n",
        "\n",
        "        all_news_data.append({'title': title, 'link': link, 'date': date})\n",
        "\n",
        "    # Move to the next page\n",
        "    page += 1\n",
        "\n",
        "    # Add a delay to prevent overloading the server\n",
        "    time.sleep(2)\n",
        "\n",
        "# Save all collected data to a CSV file\n",
        "with open(\"purdue_news_2022.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=[\"title\", \"link\", \"date\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(all_news_data)\n",
        "\n",
        "print(f\"\\nScraped {len(all_news_data)} articles in total.\")\n",
        "print(\"Data saved to 'purdue_news_2022.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhMg4axDv0qj",
        "outputId": "4debfeef-5389-486b-89c8-33d695e28c41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=1\n",
            "Scraping page 2: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=2\n",
            "Scraping page 3: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=3\n",
            "Scraping page 4: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=4\n",
            "Scraping page 5: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=5\n",
            "Scraping page 6: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=6\n",
            "Scraping page 7: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=7\n",
            "Scraping page 8: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=8\n",
            "Scraping page 9: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=9\n",
            "Scraping page 10: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=10\n",
            "Scraping page 11: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=11\n",
            "Scraping page 12: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=12\n",
            "Attempt 1 failed. Status code: 500. Retrying in 5 seconds...\n",
            "Scraping page 13: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=13\n",
            "Scraping page 14: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=14\n",
            "Scraping page 15: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=15\n",
            "Scraping page 16: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=16\n",
            "Scraping page 17: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=17\n",
            "Scraping page 18: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=18\n",
            "Scraping page 19: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=19\n",
            "Scraping page 20: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=20\n",
            "Scraping page 21: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=21\n",
            "Scraping page 22: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=22\n",
            "Scraping page 23: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=23\n",
            "Scraping page 24: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=24\n",
            "Scraping page 25: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=25\n",
            "Scraping page 26: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=26\n",
            "Scraping page 27: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=27\n",
            "Scraping page 28: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=28\n",
            "Scraping page 29: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=29\n",
            "Scraping page 30: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=30\n",
            "Scraping page 31: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=31\n",
            "Scraping page 32: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=32\n",
            "Scraping page 33: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=33\n",
            "Scraping page 34: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=34\n",
            "Scraping page 35: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=35\n",
            "Attempt 1 failed. Status code: 500. Retrying in 5 seconds...\n",
            "Scraping page 36: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=36\n",
            "Scraping page 37: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=37\n",
            "Scraping page 38: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=38\n",
            "Scraping page 39: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=39\n",
            "Scraping page 40: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=40\n",
            "Scraping page 41: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=41\n",
            "Scraping page 42: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=42\n",
            "Scraping page 43: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=43\n",
            "Scraping page 44: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=44\n",
            "Scraping page 45: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=45\n",
            "Scraping page 46: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=46\n",
            "Scraping page 47: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=47\n",
            "Scraping page 48: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=48\n",
            "Scraping page 49: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=49\n",
            "Attempt 1 failed. Status code: 500. Retrying in 5 seconds...\n",
            "Scraping page 50: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=50\n",
            "Scraping page 51: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=51\n",
            "Scraping page 52: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=52\n",
            "Scraping page 53: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=53\n",
            "Scraping page 54: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=54\n",
            "Scraping page 55: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=55\n",
            "Scraping page 56: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=56\n",
            "Scraping page 57: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=57\n",
            "Scraping page 58: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=58\n",
            "Scraping page 59: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=59\n",
            "Scraping page 60: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=60\n",
            "Scraping page 61: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=61\n",
            "Scraping page 62: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=62\n",
            "Scraping page 63: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=63\n",
            "Scraping page 64: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=64\n",
            "Scraping page 65: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=65\n",
            "Scraping page 66: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=66\n",
            "Scraping page 67: https://www.purdue.edu/newsroom/articles/?order=DESC&orderby=date&filter_year=2022&custom_post_type=post%2Cpurduetoday&paged=67\n",
            "No more articles found. Stopping pagination.\n",
            "\n",
            "Scraped 352 articles in total.\n",
            "Data saved to 'purdue_news_2022.csv'.\n"
          ]
        }
      ]
    }
  ]
}